{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e783fb4",
   "metadata": {},
   "source": [
    "# Price\n",
    "* `/v8/finance/chart/AAPL?symbol=AAPL&period1=0&period2=9999999999&interval=3mo`  \n",
    "\n",
    "#### Intervals:\n",
    "\n",
    "* `&interval=3mo` 3 months, going back until initial trading date.\n",
    "* `&interval=1d` 1 day, going back until initial trading date.\n",
    "* `&interval=5m` 5 minuets, going back 80(ish) days.\n",
    "* `&interval=1m` 1 minuet, going back 4-5 days.\n",
    "\n",
    "How far back you can go with each interval is a little confusing and seems inconsistent. My assumption is that internally yahoo is counting in trading days and my naive approach was not accounting for holidays. Although that's a guess and YMMV.\n",
    "\n",
    "`period1=`: unix timestamp representation of the date you wish to **start** at. Values below the initial trading date will be rounded up to the initial trading date.\n",
    "\n",
    "`period2=`: unix timestamp representation of the date you wish to **end** at. Values greater than the last trading date will be rounded down to the most recent timestamp available.\n",
    "\n",
    "**Note:** *If you query with a `period1=` (start date) that is too far in the past for the interval you've chosen, yahoo will return prices in the `3mo` interval regardless of what interval you requested.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23492187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import difflib\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from multiprocessing.dummy import Pool\n",
    "from datetime import datetime\n",
    "import time\n",
    "import urllib.request\n",
    "import json\n",
    "import http.client as httplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c919abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a code snippet which can tell if we have working internet connection or not.\n",
    "\n",
    "def check_internet():\n",
    "    conn = httplib.HTTPConnection(\"www.google.com\", timeout=5)\n",
    "    try:\n",
    "        conn.request(\"HEAD\", \"/\")\n",
    "        conn.close()\n",
    "        return True\n",
    "    except:\n",
    "        conn.close()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffa50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write down the function which will `get_stock_price` for given `query_url`\n",
    "\n",
    "def get_historic_price(query_url, json_path, csv_path):\n",
    "    while not check_internet():\n",
    "        print(\"Could not connect, trying again in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    stock_id = query_url.split(\"&period\")[0].split(\"symbol=\")[1]\n",
    "    \n",
    "    if os.path.exists(csv_path + stock_id + '.csv') and os.stat(csv_path + stock_id + '.csv').st_size != 0:\n",
    "        print(\"<<<  Historical data of \" + stock_id + \" already exists, Updating data...\")\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(query_url) as url:\n",
    "            parsed = json.loads(url.read().decode())\n",
    "    except:\n",
    "        print(\"|||  Historical data of \" + stock_id + \" doesn't exist\")\n",
    "        return\n",
    "    else:\n",
    "        if not os.path.exists(json_path):\n",
    "            os.makedirs(json_path)\n",
    "        if not os.path.exists(csv_path):\n",
    "            os.makedirs(csv_path)\n",
    "            \n",
    "        if os.path.exists(json_path + stock_id + '.json'):\n",
    "            os.remove(json_path + stock_id + '.json')\n",
    "        with open(json_path + stock_id + '.json', 'w') as outfile:\n",
    "            json.dump(parsed, outfile, indent=4)\n",
    "\n",
    "        try:\n",
    "            Date = []\n",
    "            for i in parsed['chart']['result'][0]['timestamp']:\n",
    "                Date.append(datetime.utcfromtimestamp(int(i)).strftime('%d-%m-%Y'))\n",
    "\n",
    "            Low = parsed['chart']['result'][0]['indicators']['quote'][0]['low']\n",
    "            Open = parsed['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "            Volume = parsed['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "            High = parsed['chart']['result'][0]['indicators']['quote'][0]['high']\n",
    "            Close = parsed['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "            Adjusted_Close = parsed['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "            df = pd.DataFrame(list(zip(Date, Low, Open, Volume, High, Close, Adjusted_Close)),\n",
    "                              columns=['Date', 'Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close'])\n",
    "\n",
    "            if os.path.exists(csv_path + stock_id + '.csv'):\n",
    "                os.remove(csv_path + stock_id + '.csv')\n",
    "            df.to_csv(csv_path + stock_id + '.csv', sep=',', index=None)\n",
    "            print(\">>>  Historical data of \" + stock_id + \" saved\")\n",
    "            return\n",
    "        except:\n",
    "            print(\">>>  Historical data of \" + stock_id + \" exists but has no trading data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae5d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter country names separated by comma (leave blank for all countries): India\n",
      "Enter desired company list separated by comma: State Bank of India, Tata Motors\n",
      "<<<  Historical data of TATAMOTORS.NS already exists, Updating data...<<<  Historical data of SBIN.NS already exists, Updating data...\n",
      "\n",
      ">>>  Historical data of SBIN.NS saved\n",
      ">>>  Historical data of TATAMOTORS.NS saved\n",
      "All downloads completed !\n"
     ]
    }
   ],
   "source": [
    "# get the stock datas with multithreading.\n",
    "\n",
    "def get_stock_data(country_names=None, desired_company_list=None):\n",
    "    if country_names is None:\n",
    "        \n",
    "        # If no country names are provided, retrieve data for all countries\n",
    "        country_names = []\n",
    "\n",
    "    if desired_company_list is None:\n",
    "        desired_company_list = []\n",
    "\n",
    "    # Load ticker data\n",
    "    ticker_file_path = \"Assets\" + os.sep + \"Yahoo Ticker Symbols - September 2017.xlsx\"\n",
    "    temp_df = pd.read_excel(ticker_file_path)\n",
    "    temp_df = temp_df.drop(temp_df.columns[[5, 6, 7]], axis=1)\n",
    "    headers = temp_df.iloc[2]\n",
    "    df = pd.DataFrame(temp_df.values[3:], columns=headers)\n",
    "\n",
    "    # Filter ticker data based on country names\n",
    "    if country_names:\n",
    "        new_df = df[df[\"Country\"].str.lower().isin([name.lower() for name in country_names])]\n",
    "    else:\n",
    "        new_df = df  # Retrieve data for all countries\n",
    "\n",
    "    # Get ticker list for desired companies\n",
    "    ticker_list = []\n",
    "    for company in desired_company_list:\n",
    "        try:\n",
    "            exact_company_name = (difflib.get_close_matches(company, new_df['Name'])[0])\n",
    "            ticker_for_the_company = new_df.loc[new_df['Name'] == exact_company_name, 'Ticker'].iloc[0]\n",
    "            ticker_list.append(ticker_for_the_company)\n",
    "        except:\n",
    "            print(\"Company name \" + company + \" not found.\")\n",
    "\n",
    "    # Create query URLs for stock tickers\n",
    "    query_urls = []\n",
    "    for ticker in ticker_list:\n",
    "        query_urls.append(\n",
    "            \"https://query1.finance.yahoo.com/v8/finance/chart/\" + ticker + \"?symbol=\" + ticker + \"&period1=0&period2=9999999999&interval=1d&includePrePost=true&events=div%2Csplit\")\n",
    "\n",
    "    # Set paths for saving JSON and CSV files\n",
    "    json_path = os.path.join(\"historic_data\", \"json\") + os.sep\n",
    "    csv_path = os.path.join(\"historic_data\", \"csv\") + os.sep\n",
    "\n",
    "    # Get stock data using multithreading\n",
    "    with Pool(processes=len(query_urls)) as pool:\n",
    "        pool.starmap(get_historic_price, zip(query_urls, itertools.repeat(json_path), itertools.repeat(csv_path)))\n",
    "    print(\"All downloads completed !\")\n",
    "\n",
    "# Example usage:\n",
    "country_names_input = input(\"Enter country names separated by comma (leave blank for all countries): \")\n",
    "desired_company_list_input = input(\"Enter desired company list separated by comma: \")\n",
    "\n",
    "country_names = country_names_input.split(',') if country_names_input else None\n",
    "desired_company_list = desired_company_list_input.split(',')\n",
    "\n",
    "get_stock_data(country_names, desired_company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e94473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0df84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
